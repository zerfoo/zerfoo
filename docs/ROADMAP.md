Thanks! I’ll produce a milestone-based product roadmap that leads from the current Zerfoo scaffold to a feature-complete version, prioritizing transformer-based models like Gemma 3.

The roadmap will:

* Be organized by major milestones (not time-based)
* Begin with core foundational components and progress linearly
* Emphasize high test coverage
* Build toward supporting transformer architecture and distributed training

I'll share the complete roadmap shortly.


# Zerfoo Project Development Roadmap (Go-Native ML Framework)

This roadmap outlines a sequence of major development milestones for the Zerfoo project, progressing from a basic scaffolded codebase to a feature-complete Go-native ML framework capable of training transformer-based models (e.g. Google’s Gemma 3). Each milestone builds on the previous one in a single-threaded path, emphasizing foundational infrastructure first and advanced model features later. High test coverage is expected at every stage to ensure reliability.

## Milestone 1: Core Infrastructure and Tensor Operations

**Goal:** Establish the fundamental project structure, tensor data type, and hardware abstraction layer (HAL) for basic numeric operations. This milestone sets up the backbone on which all higher-level ML components will rely.

* **Major Deliverables & Packages:**

  * **Project Scaffold Completion:** Finalize the Go module structure with clear packages (e.g. `tensor`, `context/hal`, `layers`, `autodiff`). Define key interfaces and types (such as a `Tensor` struct and a `Context` interface for backend operations) to outline the framework’s architecture.
  * **Tensor Data Structure:** Implement a `Tensor` type to hold multi-dimensional data and metadata (shape, dtype, etc.). Provide methods for tensor creation (from Go slices or zeros/ones initializers), shape manipulations, and basic indexing/slicing. Internally, store data in a Go slice or Gonum matrix for efficient computations, aligning with Go’s memory layout conventions.
  * **Hardware Abstraction Layer:** Define a `Context` (or `TensorOps`) interface encapsulating fundamental tensor operations like element-wise add, matrix multiplication, transpose, etc. Implement a **CPU backend** that satisfies this interface using Go’s standard libraries and Gonum BLAS for optimized math routines. For example, a `CPUContext.MatMul(A, B)` can leverage Gonum’s matrix multiplication under the hood. Design the API so that switching to a different backend (GPU, TPU) only requires swapping the context, with no changes to model-building code.
  * **Initial Utility Functions:** Provide utility routines for random number generation (weight initialization) using Go’s `math/rand`, and logging or error-checking helpers (e.g. shape assertions) to ease development and debugging.

* **Implementation Notes:** Embrace Go idioms by favoring composition over inheritance for extensibility. Keep the code simple and explicit—e.g. prefer straightforward function calls and interfaces to complex generics or reflection. The `Context` interface should be minimal yet extensible, allowing **dependency injection** of the compute backend into layers and models. Ensure that the context is passed or embedded where needed so all tensor operations funnel through it (this will later enable GPU/TPU integration without changing model code). By the end of this milestone, the framework should compile and have a clear directory layout, establishing a strong foundation for subsequent milestones.

* **Testing Focus:**

  * Verify basic tensor creation and properties (shapes, element values) work as expected. For example, create a tensor from a known slice and confirm that indexing returns the correct values.
  * Test a few fundamental `Context` operations on small, hard-coded tensors. e.g., matrix multiplication of two 2x2 matrices with known values should produce the correct result on the CPU backend. Similarly test element-wise operations like add/subtract with simple inputs.
  * Ensure error cases are handled (e.g., shape mismatches in operations should return clear errors instead of panics). Include tests that intentionally misuse the API to confirm it fails gracefully.

## Milestone 2: Automatic Differentiation Engine (Reverse-Mode AD)

**Goal:** Build the automatic differentiation (autograd) system to enable gradient computations via backpropagation. This milestone introduces the ability to record operations in a computation graph or tape and compute gradients, a critical feature for training neural networks.

* **Major Deliverables & Packages:**

  * **Computation Graph/Tape:** Design a structure to record the sequence of tensor operations so that gradients can be computed. This could be a **graph of nodes** (each node representing an operation or variable) or a tape (list) of operations recorded during the forward pass. Each `Tensor` will be associated with a graph node tracking how it was produced.
  * **Operation Interface:** Define an `Op` interface representing a differentiable operation with methods like `Forward(inputs) -> output` and `Backward(inputs, gradOutput) -> gradInputs`. Additionally, implement a specialized `ADOp` or similar interface for ops that support automatic differentiation, providing a function to compute gradients (this follows the design where each primitive operation has an explicit gradient function). Register gradient formulas for core ops (e.g., addition’s gradient is 1 for each input, matmul’s gradient uses the transpose of the other matrix, etc.).
  * **Gradient Computation:** Implement reverse-mode AD. After a forward pass, the framework should allow computing gradients by traversing the recorded operations in reverse order. This includes storing intermediate values needed for gradient calculation (e.g., store inputs or outputs of operations if required for the backward formula). For example, for a matrix multiply node $C = A \times B$, store $A$ and $B$ so that when computing $\nabla A$ and $\nabla B$, those values are available.
  * **Basic Differentiable Operations:** Modify the previously implemented tensor operations to integrate with the autograd system. For each operation (add, matmul, etc.), ensure that performing it in a context of a “recording” graph creates a node and attaches the appropriate gradient function. For instance, calling `Tensor.Add()` not only produces a result but also records an `AddOp` in the graph linking the input node(s) to the output node.

* **Implementation Notes:** This autograd system should be **idiomatic Go** in design. Rather than relying on global static graphs as in Python frameworks, consider using explicit graph objects or context-bound tapes. For example, a `Graph` object could be created for each model or training run, and tensors carry references to their graph nodes. Use Go’s interfaces to keep the system flexible (e.g., allow different `Op` implementations) and use struct embedding or simple containers to manage the graph. Keep thread-safety in mind: while single-threaded execution is the focus now, structuring the graph to be immutable after construction or accessed via channels could ease future parallelization. Ensure memory efficiency by freeing graph data when no longer needed (possibly provide a method to reset or clear the tape each iteration, analogous to PyTorch’s computation graph that is dynamic by default).

* **Testing Focus:**

  * Validate gradient computations for simple scenarios. For example, create a scalar function $y = x^2 + 3x$ and check that the computed gradient at various x values matches the analytical result $dy/dx = 2x + 3$.
  * Test a small multi-step computation: e.g., $z = \text{sum}(A \times B)$ for small 2x2 matrices A and B. Compute gradients w\.r.t A and B and compare against numerical approximations for correctness.
  * Ensure that the graph/tape properly resets between runs (to prevent gradients from accumulating from previous computations unless intended). Also test that no gradient is recorded when the user marks a value as non-trainable (if such a feature is planned for constants or inference mode).
  * If using a tape-based approach, test that memory usage does not blow up over iterations (simulate a training loop of a few steps and ensure old operations are not retained unintentionally).

## Milestone 3: Parameter Management and Optimization Utilities

**Goal:** Introduce a systematic way to handle model parameters (weights/biases) and their gradients, and implement basic optimization algorithms for training. This milestone links the autograd engine with trainable parameters and provides optimizers like SGD/Adam to update those parameters based on computed gradients.

* **Major Deliverables & Packages:**

  * **Parameter & Gradient Tracking:** Develop a `Parameter` struct or designate a tensor flag to mark trainable parameters. Each parameter tensor should maintain a reference to its gradient tensor, establishing an intrinsic link between them. For example, if `W` is a weight matrix, there should be an associated `dW` that autograd populates during backpropagation. Implement a global or model-level **Parameter Store** to register all trainable parameters for easy iteration during updates. This could be as simple as a slice or map of parameter tensors, or part of a higher-level `Model` struct.
  * **Optimizers:** Provide an interface (e.g., `Optimizer` with method `Step()` or `Update(param, grad)` methods) and implement common optimization algorithms. Start with **Stochastic Gradient Descent (SGD)** with learning rate, then extend to **Momentum**, **Adam**, and others as needed. The optimizer should retrieve each parameter and its gradient from the Parameter Store and adjust the parameter in place according to the update rule. For instance, implement Adam by maintaining moving average buffers for gradients and squared gradients inside the optimizer struct.
  * **Integration with Context (Device Awareness):** Ensure that parameter initialization, storage, and updates respect the `Context`. If a model is on CPU vs. GPU, the parameters and gradients should reside in the respective memory, and optimizer computations (which are simple element-wise ops) should also utilize the context’s operations. This may involve extending the Context interface with methods for in-place updates or incorporating small CPU-bound calculations (like incrementing optimizer moment estimates) carefully so as not to bottleneck GPU training.
  * **Learning Rate Scheduler (Optional):** As an added utility, include a basic learning rate scheduling mechanism (e.g., reduce LR after certain epochs or a decay schedule). This can be part of the optimizer configuration for completeness, though not strictly required for correctness.

* **Implementation Notes:** Align with Go’s explicitness and type-safety. Rather than using reflection or magic, explicitly register parameters (e.g., a model constructor could return a list of parameters or auto-populate a slice of parameters if using struct tags). The parameter-graduate linkage can be achieved by embedding a gradient field in the `Tensor` struct or via a parallel map from parameter IDs to gradient tensors. Ensure **zero-initialization of gradients** at appropriate times (e.g., zero-out gradients at the start of each backward pass or each training iteration to avoid accumulation, unless accumulating across batches intentionally). For optimizers, favor clear struct fields for hyperparameters (learning rate, beta values for Adam, etc.) and document their usage. Implementations should be thoroughly tested on simple problems before relying on them for complex models.

* **Testing Focus:**

  * **Gradient Linking:** Verify that after a backward pass, each Parameter’s gradient field is populated correctly. For example, if a model has 2–3 parameters, perform a forward and backward pass and check that each parameter now has a non-nil gradient tensor of the same shape with expected values (numerically validate on a tiny model).
  * **SGD Update:** Manually compute one step of SGD on a small model (e.g., a single weight neuron) and check that the code’s `Optimizer.Step()` updates the weight as expected. Do the same for Adam: for a simple known gradient sequence, ensure that the Adam implementation’s updated weights match a reference Python implementation or known formula.
  * **Multiple Iteration Training:** Simulate a few training iterations on a trivial dataset (like fitting a line $y = mx + b$). Confirm that the loss decreases with each optimizer step, indicating that gradients are being applied correctly to parameters.
  * **Edge Cases:** Test optimizers with extreme hyperparameters (very high or zero learning rate) to ensure no crashes or NaNs, and that parameter updates still occur as coded. If a Parameter is not used in a computation (thus has no gradient), ensure the optimizer safely skips it without error.

## Milestone 4: Fundamental Neural Network Layers and Loss Functions

**Goal:** Build out the basic neural network layers and loss functions using the infrastructure from previous milestones. This milestone delivers essential layer types (dense layers, activation functions, etc.) and cost functions needed to assemble and train actual models.

* **Major Deliverables & Packages:**

  * **Dense (Linear) Layer:** Implement a fully-connected layer that produces `Y = XW + b`. This `Linear` layer struct should hold its weight matrix `W` and bias vector `b` as `Parameter` tensors. It will use the `Context` operations for the matrix multiply and vector addition. Ensure that using this layer in a computation correctly registers `W` and `b` in the parameter store and that gradients flow into these parameters during backpropagation.
  * **Activation Functions:** Provide common activation layers or functions – e.g. **ReLU**, **Sigmoid**, **Tanh**, etc. These can be implemented as layer structs with no trainable parameters but with a `Forward()` that applies the non-linearity (through context ops) and a `Backward()` that uses the stored input to compute gradients (for example, ReLU backward uses an indicator of input > 0). Some activations (like Softmax) span across vector outputs; implement **Softmax** as well, since it’s crucial for transformer attention and for classification outputs. Softmax can be provided as a function that operates on a tensor’s last dimension or as part of a combined layer (e.g., SoftmaxCrossEntropy loss).
  * **Loss Functions:** Implement common loss functions needed for training. This includes **Mean Squared Error (MSE)** for regression and **Cross-Entropy Loss** for classification (with a variant that combines Softmax + Cross-Entropy for numerical stability). Loss functions can be implemented as special layers or functions that take predicted vs. target outputs and produce a scalar loss tensor. They should integrate with autograd (so that calling loss on predictions records operations whose backward pass yields gradients on the predictions).
  * **Utility Layers:** If applicable, implement other utility layers such as **Dropout** (to randomly mask out activations during training), though dropout’s effect is mostly during training and it doesn’t have trainable parameters. Also consider a **Batch Normalization** layer for completeness (even if transformers typically use LayerNorm, BatchNorm is useful for other models). BatchNorm would include trainable scale and shift and maintain running mean/var (though the state handling might be complex; it can be deferred or simplified if focusing on transformer use-case where LayerNorm is preferred).

* **Implementation Notes:** Use Go’s **composition** patterns to combine simple operations into these layers. For example, the Linear layer can embed or contain its weight and bias, and possibly embed a simple struct that implements the `Layer` interface for the forward/backward logic. Activation layers might embed no fields but satisfy the interface by wrapping an op. Aim for a common `Layer` interface (with methods like `Forward(x Tensor) Tensor` and `Backward(dy Tensor) Tensor`) so that layers can be used interchangeably and composed in models. Leverage the existing autograd: e.g., the ReLU `Forward` should call a `context.ReLU(x)` operation that was added to the Context’s ops with a known gradient. For loss functions, ensure they produce a scalar tensor that the autograd engine can differentiate through to model parameters. Keep these implementations efficient – e.g., computing softmax and cross-entropy in one pass to avoid numerical issues and extra passes over data. Follow Go idioms by keeping APIs consistent (e.g., `Forward` should not modify layer state, only use inputs to produce outputs, except for layers like BatchNorm that track running stats).

* **Testing Focus:**

  * **Layer Forward/Backward:** Unit-test each layer’s forward and backward in isolation. For Linear, manually compute a forward pass on a small input and compare with output. Then provide a known gradient on the output (e.g., all ones) and verify the backward pass returns the correct gradients for weights, bias, and input (compare with numerical gradient if needed).
  * **Activation Correctness:** Test ReLU on a mix of positive/negative inputs to ensure output is correct and gradient is zero for negative parts. Test Sigmoid’s output and gradient against known values (e.g., sigmoid(0) = 0.5, gradient = 0.25). For Softmax, test that outputs sum to 1 and that a backward pass through a combined softmax-cross-entropy yields the expected gradient (for a simple 2-class problem, the gradient should be $p - \text{onehot}(target)$).
  * **Loss Functions:** Verify MSE loss on a simple prediction vs target (e.g., pred=2, target=5 should give loss 9 and gradient -6). Verify Cross-Entropy loss on a single data point with a known probability distribution for the prediction. Ensure no instability for large/small input values (test that softmax doesn’t produce NaNs on extreme inputs).
  * **Integration Test:** Build a minimal neural network (e.g., one Linear layer followed by Sigmoid for binary classification) and train it on a tiny dataset (like XOR or a few points) for a few epochs. Check that the loss decreases and the final predictions are correct. This end-to-end test validates that layers, loss, autograd, and optimizer work together properly.

## Milestone 5: Model Assembly and Training Workflow API

**Goal:** Provide mechanisms to assemble layers into complete models and utilities to streamline the training loop. This milestone shifts focus from individual components to the overall user experience of defining a model and training it with data.

* **Major Deliverables & Packages:**

  * **Model Composition Interface:** Develop a clean way for users to define models by composing layers. One approach is to leverage Go’s **struct embedding** to compose a model from layer fields. For example, define a struct `MyModel` that embeds layers as fields (e.g., `Layer1 Linear`, `Layer2 Linear`, `Act Activation`). By satisfying a common `Layer` or `Model` interface (with `Forward()` method), the model struct can itself act like a layer. Alternatively, provide a `Sequential` type where users can append layers, and `Forward` loops through them in order. In either case, ensure that all parameters from sub-layers are accessible (e.g., via the Parameter Store or by an iteration method) so that the optimizer can update the whole model in one go.
  * **Training Loop Utilities:** Implement functions or an API to facilitate training. For example, provide a `Train(model, optimizer, lossFn, dataLoader, epochs)` function that handles the boilerplate of iterating over epochs and batches: for each batch, calls `model.Forward` on the inputs, then the `lossFn`, then triggers `Backward` on the loss and finally calls `optimizer.Step()`. This can be kept flexible (perhaps by using interfaces for data loaders or callbacks for logging). The aim is to standardize training procedures and reduce user error.
  * **Model Saving/Loading:** Introduce a mechanism to save a model’s learned parameters to disk and load them back. This could be as simple as writing all parameters to a file (using gob or JSON for simplicity, or a custom binary format for efficiency). This feature, while not directly related to training, becomes important for checkpointing models like transformers that can take days to train. Ensure that any saved file captures all necessary data (weights, perhaps optimizer state if needed) but not the graph (the graph can be rebuilt on reload).
  * **Example Models:** As part of this milestone, deliver a couple of example model definitions using the framework. For instance, a small **feed-forward classifier** (combining Linear layers and ReLU) or a simple **convolutional network** (if a Conv layer is easy to add using composition of Linear operations, though conv could be deferred if focusing on transformers). These examples will serve both as documentation and as integration tests of the modeling API.

* **Implementation Notes:** Focus on making the API ergonomic for Go developers. The model composition should use clear Go patterns rather than mimic dynamic languages. Struct embedding is powerful here: by embedding layers that each have their own `Forward`, the model’s `Forward` can explicitly call sub-layers’ forward in sequence (since there is no automatic magic – unlike some Python frameworks, Go will not automatically compose layers, but we can write a little boilerplate or use reflection on struct fields if we wanted to auto-discover layers by type or tag). Given Go’s preference for code generation in complex scenarios, one could even use `go generate` or templates to reduce repetitive code in model definitions (this aligns with the design’s mention of using `text/template` for code generation in model definitions, though a full UI-assisted codegen is beyond this milestone’s scope). Keep training loop code simple and safe: use Go’s `for` loops and range over data slices or channels (if using a channel-based data loader). Leverage concurrency carefully – for example, one might use goroutines to load the next batch while the current batch is computing, but avoid prematurely optimizing unless needed for throughput. This milestone is also a good point to refine error handling and ensure any panics in forward/backward are caught (perhaps by returning errors up the call stack of Forward/Backward).

* **Testing Focus:**

  * **Model Forward Integration:** Test that a composed model (with multiple layers) produces correct outputs. For example, if `Model = Linear1 -> ReLU -> Linear2`, compare its output on a sample input to the manually computed result of applying each layer in turn. This ensures the model’s forward pass correctly invokes sub-layer forwards in order.
  * **Training Loop End-to-End:** Use a small synthetic dataset (e.g., random points classified by a simple known boundary) and run the provided `Train` utility for a few epochs. Ensure that losses decrease and final accuracy is reasonable. This test should cover the end-to-end flow: data loading, forward pass, loss calculation, backward pass, and parameter update. Any discrepancy (like no improvement in loss) could indicate an issue in how gradients propagate through the model or how the optimizer is applied.
  * **Concurrency and Memory:** If any goroutines are used (for data loading or parallel forward passes), test that there are no race conditions. Use Go’s race detector in tests to catch any unintended concurrent memory access when running the training loop. Also monitor memory usage in a long-ish training simulation to check for leaks (for example, ensure that we don’t accumulate computation graphs without releasing them each iteration).
  * **Persistence:** Test saving and loading of a model. Train a model for a couple of iterations, save the weights, manually alter the model (to ensure you’re not inadvertently using the same in-memory pointers), then load from disk and verify that the model’s predictions are identical before and after saving. This confirms that all necessary state is being saved and restored.

## Milestone 6: Advanced Transformer Components (Multi-Head Attention, LayerNorm, Positional Encoding)

**Goal:** Implement the key building blocks specifically required for transformer-based models. This milestone focuses on components like multi-head self-attention, layer normalization, and positional encodings, which are essential for architectures such as the Google Gemma 3 model (a multimodal Transformer with a large context window).

* **Major Deliverables & Packages:**

  * **Embedding Layer:** Introduce an `Embedding` layer for handling discrete token inputs. This layer maintains a trainable lookup table of shape `[vocabSize x embedDim]`. Implement `Forward(indices)` to produce the sequence of embedding vectors (e.g., by gathering rows corresponding to token IDs). Leverage efficient indexing – for CPU, a simple loop over indices to pull from an array; for GPU later, a scatter/gather kernel or a batched one-hot matmul approach. Ensure embeddings are marked as parameters so they update with gradients.
  * **Positional Encoding:** Provide a mechanism to inject positional information into sequence embeddings. Implement **Sinusoidal Positional Encoding** as in the original Transformer (compute fixed sin/cos patterns for positions) or a trainable positional embedding vector per position. For sinusoidal, create a function to generate a position matrix of shape `[maxSeqLen x embedDim]` and add it to token embeddings. If trainable, treat it as an `Embedding` with `maxSeqLen` entries. Make sure this operates through the `Context` for addition. This component has no effect on shapes beyond adding to the embedding tensor.
  * **Layer Normalization (LayerNorm):** Implement `LayerNorm` as a layer that normalizes the input across the feature dimension for each example, and has learnable scale (`gamma`) and shift (`beta`) parameters. Include an epsilon constant for numerical stability. `Forward(x)` should compute $\text{LayerNorm}(x) = \gamma \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$, where \$\mu\$ and \$\sigma^2\$ are the mean and variance of the elements in each input sample (for transformer, typically across the embedding dimension). Implement `Backward` to propagate gradients through the normalization (this is a bit involved; ensure to get the partial derivatives correct or test with automatic checks). Both `gamma` and `beta` are trainable parameters.
  * **Multi-Head Attention (MHA):** This is the centerpiece. Implement a `MultiHeadAttention` layer that encapsulates the full self-attention mechanism. Design it to accept three inputs: Query (Q), Key (K), and Value (V) sequences – for self-attention, Q, K, V are all the same sequence, but the layer should also work for cross-attention if needed by allowing different K/V. Key components:

    * Trainable projection layers for Q, K, V: typically these are Linear layers that reduce the embedding dimension to `d_k` (the dimension of each head). If the model dimension is `d_model` and number of heads is `h`, then `d_k = d_model/h`. Create three Linear sub-layers (or one layer that does all three via concatenation) to project inputs to Q, K, V of shape `[seqLen x d_model] -> [seqLen x d_k]` for each head.
    * Attention scoring and weighting: Implement the scaled dot-product attention. Split Q, K, V into `h` heads (this can be done by reshaping/tensor views if supported, or by performing separate linear projections per head). Compute attention scores as `score = Q * K^T / sqrt(d_k)`. Use the Softmax operation to turn scores into probabilities. Then compute the weighted sum: `attention = softmax(score) * V`.
    * Output projection: All heads’ attention outputs (each of shape `[seqLen x d_k]`) should be concatenated back into a `[seqLen x d_model]` tensor, then passed through a final trainable Linear layer to mix head information.
    * Package this logic inside `MultiHeadAttention.Forward(query, key, value)`, performing the above steps. Ensure all internal linear layers and the final projection are registered parameters. Leverage existing ops (matmul, softmax) via the context for performance. This layer will be built as a composite: it **embeds multiple Linear layers and uses Softmax**, exemplifying the framework’s compositional design.
  * **Position-wise Feed-Forward Network:** (If not already available from earlier layers) Implement a convenience layer for the Transformer’s feed-forward sublayer. This typically is two linear transformations with an activation in between: $\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2$ (using ReLU). You might have the components already (Linear and ReLU), but for convenience, wrap them in one `FeedForward` struct that applies Linear1 -> ReLU -> Linear2 in its `Forward`. This just makes transformer block assembly cleaner and mirrors the typical architecture.

* **Implementation Notes:** Pay special attention to aligning with Go’s memory management and performance characteristics here. For example, when splitting into multiple heads, you might choose to physically split the tensor (which could allocate new memory) versus using views/offsets. In Go, slicing can give a view on one dimension of a slice, but for 2D/3D tensors, careful calculation of indices is needed. Given this complexity, an alternate approach is to perform multi-head attention by looping over heads in Go code (which is simpler but not as fast) or by vectorizing the operations where possible (e.g., combine Q matrices for all heads into one big matrix and do a single matmul if the library supports batched matmul). Start with clarity (even if it means a loop per head), then optimize as needed. Utilize the framework’s existing pieces: e.g., the Linear projections are instances of the Linear layer from Milestone 4 (reused here), and Softmax op from Milestone 4 is reused for attention weights. This re-use underscores the compositional design (no new complex math code, just arranging existing operations). Also ensure that the new components (LayerNorm, MHA) integrate with autograd properly — their forward passes use existing differentiable ops or clearly implement Backward. Keep the API of MHA user-friendly: e.g., allow it to be called with a single input sequence for self-attention (internally it can use that for Q, K, V).

* **Testing Focus:**

  * **Positional Encoding:** If using sinusoidal, test that the generated positional encodings match known values from the formula (for small positions, you can compute a few values by hand or compare to reference). If trainable, test that they are indeed being treated as parameters by checking they appear in the parameter list and get updated when training (you could simulate a training step and see a change). Also verify that adding positional encodings doesn’t alter shapes and that the addition is correctly applied across a batch of sequences.
  * **LayerNorm:** Confirm that LayerNorm zero-centers and unit-normalizes inputs. For a simple input vector, compute mean and variance manually and ensure the layer’s output is correct (and that scaling and shifting by gamma/beta works). Backpropagate a gradient of 1 through the layer and verify the gradients for gamma, beta, and input make sense (numerical approximation can help, as LayerNorm’s backward is intricate). Also test behavior on edge cases (like constant input vectors, where variance is 0 – the epsilon should prevent division by zero).
  * **Multi-Head Attention:** This is complex, so break down testing:

    * **Internal Consistency:** Fix the weights of the Q, K, V linear layers and the output projection to known matrices (even identity or random but recorded) and run a forward pass with a simple known input (like an identity matrix or a sequence of one-hot vectors). Manually compute the expected attention result for one head to verify the computation. For instance, with identity weights, MHA should essentially perform attention on the raw input: check that the output equals input in a trivial case or follows an expected distribution for a crafted case.
    * **Gradient Test:** Use numerical gradient checking on MHA: slightly perturb an input element and see if the change in loss matches the gradient from backprop. This can be done on a very small scale (e.g., 1 head, 2-dimensional vectors) due to the heavy computation. It will validate that all pieces (including the softmax gradient) are wired correctly.
    * **Multi-Head vs Single-Head:** If you configure MHA with 1 head and compare it to a reference single-head attention (or even a simpler manually coded attention), they should produce the same result. This checks that the splitting/concatenation logic doesn’t alter the outcome.
  * **Integration in Transformer Block:** As a precursor to the next milestone, you might assemble a single transformer block (MHA + LayerNorm + FFN + LayerNorm) and test it on a small sequence to ensure it runs end-to-end (even randomly). This ensures all new components can connect without shape or interface issues.

## Milestone 7: Assembling the Transformer Model (Feature-Complete Model)

**Goal:** Combine the developed components into a full transformer-based model and demonstrate that the framework can train such a model end-to-end. By the end of this milestone, Zerfoo should be able to define and train models akin to Google’s Gemma 3 (within resource limits), validating that all necessary features work in concert.

* **Major Deliverables & Packages:**

  * **Transformer Block and Model Definition:** Create a `TransformerBlock` struct that encapsulates the typical Transformer subcomponents: one Multi-Head Self-Attention layer (with its associated LayerNorm and skip connection) followed by a Feed-Forward network (with its LayerNorm and skip connection). Organize it such that `Forward(x)` performs: `y = x + MHA(LN1(x))` then `z = y + FFN(LN2(y))` (this order: pre-norm formulation). Each block will embed its sub-layers (MHA, FFN, and two LayerNorms) so they are part of the block’s state.
  * **Configurable Transformer Model:** Implement a `TransformerModel` struct that composes multiple `TransformerBlock` instances (e.g., a slice or array of them) along with input embedding layers. For example, `TransformerModel` could have fields `TokEmbed Embedding`, `PosEmbed PosEncoding`, and an array `Blocks []TransformerBlock`, plus perhaps an output projection layer (for language modeling, an output linear tied or untied to input embedding). Provide flexibility in configuration: number of layers, number of heads, model dimension, etc., so that various sizes (small for testing, larger for real) can be instantiated. The model’s `Forward(inputTokens)` should apply the token embedding, add positional encodings, then loop through each TransformerBlock’s forward, and finally produce an output (e.g., logits for each token position if doing language modeling).
  * **Training Example for Transformer:** Prepare a demonstration training scenario for the transformer model. For instance, a **language modeling** task on a small dataset (even a toy dataset of sentences or a subset of Wiki text) or a **sequence-to-sequence** task. This serves both as a proof that the model trains and as a usage example. It will likely require implementing a loss function suitable for sequence outputs (if not already, e.g., cross-entropy over vocabulary at each time step) and perhaps an accuracy metric.
  * **Documentation & Tutorials:** As the model is complex, include comprehensive documentation or a tutorial in the repository for how to use Zerfoo to build and train a transformer. This may be a README section or Go doc comments that walk through constructing a TransformerModel and training it. Highlight how Zerfoo’s Go-native approach (struct composition, etc.) is used in defining the model.

* **Implementation Notes:** The assembly of a full transformer is a **stress test of the framework’s design**. Pay attention to any friction: e.g., if connecting sub-layers requires a lot of boilerplate, consider adding helper functions (perhaps a function to create a standard transformer block given dimension params). Ensure that the model construction does not overly burden the user with manual wiring – the interfaces and struct embedding should make it straightforward. Take advantage of Go’s compile-time checks: for instance, if a user forgets to add a layer to the parameter store, the model’s training might silently not update that layer. To avoid this, you could implement in the `TransformerModel` a method like `Parameters()` that aggregates parameters from all sub-layers (iterating over blocks and collecting their params). This could be done manually or via reflection on fields tagged as parameters. Efficiency considerations: for large models like Gemma 3 (billions of parameters), the framework needs to handle memory well. Ensure that operations like the skip connection (`x + MHA(x)`) do not unnecessarily copy data (in-place add if possible via context operations to save memory). Also, consider gradient accumulation memory: a large model’s backward will need memory for gradients of every layer – releasing intermediate activations as soon as they are used in backward (if using a tape, maybe free them) can keep memory usage in check. At this point, you may start to incorporate some **concurrency** for performance: e.g., use goroutines to compute different attention heads or different layers in parallel when feasible (though proper synchronization is needed). Go’s ability to easily spawn goroutines means the user could parallelize certain independent operations; the framework could provide options or recommendations for doing so when training very large models.

* **Testing Focus:**

  * **Small-Scale Transformer Training:** Construct a miniature transformer (e.g., 2 layers, 2 heads, small embedding size) and train it on a simple task. One effective test is a **copy task** (where the model must output the input sequence shifted, etc.) or predicting the next character in a short piece of text. Run a few epochs and check that the model learns recognizable patterns (e.g., near 100% accuracy on the copy task or decreasing perplexity on text). This validates the interplay of embedding -> MHA -> FFN -> loss.
  * **Gradient Flow in Deep Model:** With multiple layers, it’s important to ensure no layer blocks gradient flow. After a backward pass on the full TransformerModel, check that each layer’s parameters have non-zero gradients (for a random input and random labels, most parameters should get some gradient if wired correctly). This helps catch issues like forgetting to connect a sub-layer to the computation graph or missing gradient propagation through skip connections.
  * **Consistent Output Dimensions:** Test that the model handles various input lengths and that output shapes make sense. For example, feed sequences of length N and N+1 and ensure the model outputs correspondingly shaped predictions. The positional encoding should properly handle the length; if input exceeds the max positional length, ensure the code handles it (either by error or by extending the encoding on the fly).
  * **Performance Profiling:** Although full performance tuning comes next, it’s useful to measure that the model can at least run through a forward-backward pass on a moderately sized input in reasonable time. If possible, compare a forward pass of a small Zerfoo transformer to an equivalent in PyTorch or TensorFlow on CPU to gauge the overhead. This can highlight any glaring inefficiencies (like an accidental \$O(n^2)\$ loop where vectorized ops are possible).

## Milestone 8: Performance Optimization and Distributed Training Support

**Goal:** Enhance the framework’s performance and scalability so it can train large transformer models efficiently. This final milestone includes integrating GPU support, utilizing Go’s concurrency, and enabling distributed training across multiple nodes for truly large-scale models. The framework is polished for a production-ready state with high test coverage and documentation.

* **Major Deliverables & Packages:**

  * **GPU/Accelerator Backend:** Implement a **GPU backend** for the Context interface. This involves creating a `CUDAContext` (or using OpenCL or vendor libraries) that provides fast implementations of tensor operations on GPUs. Likely, use Cgo to call CUDA libraries (cuBLAS, cuDNN) or leverage existing Go GPU libraries if available. Focus on the operations critical for transformers: matrix multiplies, layernorm calculations, and large batched ops. Provide a way for users to choose this backend (e.g., `zerfoo.NewGPUContext()`) and ensure that tensors created under this context reside in GPU memory. This will drastically speed up training for large models.
  * **Multi-GPU and Distributed Training:** Extend the training capabilities to multiple GPUs and multiple machines. For multi-GPU on one machine, allow splitting the model or data across devices – e.g., implement **data parallelism** where each GPU gets a portion of the batch and computes gradients, then gradients are aggregated (All-Reduce) and applied to the parameters. Across multiple machines, implement a communication layer using **gRPC** to coordinate training. For instance, create a simple **parameter server** mode: one process is the parameter server, others are workers that compute gradients and send them to the server. Alternatively, implement an All-Reduce strategy over gRPC for a decentralized approach. The key deliverable is a `DistributedTrainer` or similar utility that orchestrates this: maybe using Go’s `sync.WaitGroup` and channels to parallelize work across nodes and aggregate results.
  * **Go Concurrency Utilization:** Refine the internal use of goroutines to speed up computations on CPU and to overlap operations. For example, on CPU backend, large independent ops (like two matmuls in different layers) could be run in parallel if resources allow. On the algorithmic side, ensure asynchronous computation where possible – e.g., issue GPU kernels without blocking (if using CUDA streams) and use goroutines to overlap data preparation with computation. The design’s emphasis on Go’s concurrency should be reflected in how Zerfoo can keep hardware busy.
  * **Robustness and Fault Tolerance:** Add features to handle real-world training interruptions and errors. For distributed training, incorporate timeouts and retries in communication using Go’s `context` package to avoid hangs. If a node fails or a network blip occurs, the system should either safely stop with a clear error or ideally recover (perhaps by retrying certain parameter syncs). Checkpointing during training is another deliverable: periodically save model state so long runs can resume after a crash or be paused and continued.
  * **Documentation and Final Polishing:** Complete the reference documentation for all public APIs, ensure all examples are up-to-date, and polish the repository (clear README, contribution guide, etc.). Write guidelines for deploying Zerfoo on different environments (e.g., how to install CUDA for the GPU backend, how to set up multi-node training with gRPC). At this stage, the framework should be ready for an initial release to the Go and ML communities.

* **Implementation Notes:** GPU integration will likely involve non-Go code; keep this minimal and isolated. For example, implement a small C/C++ shim that exposes only the needed BLAS routines or kernel launches, and use Cgo to call it. Be mindful of Go’s CGO call overhead – batch operations to amortize call costs (for instance, send one big matrix multiply rather than many small ones). Use Go’s `unsafe.Pointer` or `uintptr` wisely to avoid copying data between Go and C when transferring tensors to GPU memory. In distributed training, ensure that floating-point precision issues are considered – e.g., summing gradients across machines might need double precision or compensation (maybe beyond scope, but mention it). Also, consider security of the distributed protocol (authentication if needed when scaling beyond trusted environments). When utilizing goroutines for performance, always guard shared data with sync primitives or design work as tasks that do not share memory to avoid race conditions. Before this milestone, tests have been largely focused on correctness; now add **stress tests** for performance: train a moderately large model for a few iterations and profile CPU/GPU usage, memory usage, and throughput. Optimize identified hotspots (could be as simple as using a more efficient algorithm in one of the ops or parallelizing a loop).

* **Testing Focus:**

  * **GPU vs CPU Parity:** Run a small network’s training for a couple of iterations on CPU context and GPU context and ensure they produce *nearly* identical results (floating-point differences aside). This checks that the GPU ops are implemented correctly. Also measure the speed to confirm a substantial improvement on GPU for large tensors (e.g., a big matmul should be much faster on GPU).
  * **Distributed Training Correctness:** Set up a simulation of two or more workers (this can be done on one machine by spawning multiple Go routines or processes that communicate via gRPC loopback). Train a simple model for a few steps in distributed mode and compare the result to training the same model on a single node for the same number of total updates. They should match closely or exactly (if using synchronous updates, the match can be exact). Test edge conditions like one worker running slower (simulate by sleeping) to see that the others wait or the system handles the sync properly.
  * **Failure Recovery:** Simulate a fault in distributed training (e.g., one worker dropping out). The system should either gracefully shut down informing the user, or if possible, handle it (maybe by redistributing work or pausing). Test that a saved checkpoint can be re-loaded and training can continue from that point with identical results as if no interruption occurred.
  * **Load Testing:** Push the framework to its intended limits. For example, instantiate a model with a number of parameters comparable to a smaller Gemma 3 variant (say 1B parameters across 24 layers). This will test memory handling and any scalability issues. Even if you cannot train such a model to convergence, ensure that at least one forward/backward pass can run on a high-memory machine or multi-GPU setup. Monitor for any overflow in indexing (e.g., int32 vs int64 for indexing large tensors) and for any operations that become painfully slow at this scale.
  * **Comprehensive Test Suite:** By this final milestone, aim for very high test coverage. Include tests for all modules (context operations, autograd edge cases, each layer type, integration tests for training loops, etc.). Use continuous integration tools to run the test suite on different platforms (linux, mac, windows) and with different Go versions to ensure portability. The final deliverable is a fully tested Zerfoo framework ready for broad use.

---

**Outcome:** Upon completing these milestones, Zerfoo will be a fully functional, Go-idiomatic machine learning framework, capable of training modern transformer-based models from scratch. It will support all necessary components – from low-level tensor ops with automatic differentiation, up through high-level modules like multi-head attention – tested and optimized on both CPU and GPU. With distributed training capabilities, Zerfoo can scale to the demands of large models such as Google’s Gemma 3, which features multimodal inputs and large context windows. Throughout development, a focus on composability and alignment with Go’s strengths (simplicity, concurrency, strong typing) ensures that the framework is not only powerful but also approachable for Go developers. The final product enables researchers and engineers to experiment with advanced AI models in pure Go, enjoying performance optimizations and scalability without sacrificing the clarity and robustness of Go’s programming model.

**Sources:** The roadmap above is informed by the Zerfoo architectural design document, which emphasizes a two-layer abstraction (hardware independence and compositional layers) and core features like automatic differentiation, parameter management, and distributed training interfaces. These principles guide each milestone, ensuring that the development steps align with the overarching vision for a disruptive Go-native ML framework for AGI experimentation.
